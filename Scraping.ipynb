{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T14:41:09.374285Z",
     "start_time": "2018-01-28T14:41:06.954011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup Scraping\n",
    "* Scraping pro-football reference for all players and associated URLs\n",
    "* Taking only quaterbacks\n",
    "* Use these names to find the associated college quarterback records\n",
    "* Use these URLs to retrieve NFL player stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:03:09.564089Z",
     "start_time": "2018-01-17T02:03:09.557360Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_players = {}\n",
    "dict_players['player'] = []\n",
    "dict_players['url'] = []\n",
    "dict_players['pos'] = []\n",
    "\n",
    "for letter in string.ascii_uppercase:\n",
    "    t0 = time.time()\n",
    "    url_p = 'https://www.pro-football-reference.com/players/' + letter + '/'\n",
    "    \n",
    "    response_p=requests.get(url_p)\n",
    "    page_p=response_p.text\n",
    "\n",
    "    soup_p=BeautifulSoup(page_p,\"lxml\")\n",
    "\n",
    "    for row in soup_p.find_all('p'):\n",
    "        if row.find('a') is not None:\n",
    "            if row.find('a').get_attribute_list('href')[0][:9] == '/players/':\n",
    "                dict_players['player'].append(row.find('a').text)\n",
    "                dict_players['url'].append(row.find('a').get_attribute_list('href')[0])\n",
    "                dict_players['pos'].append((re.search(r'\\((.*?)\\)',row.text).group(1)))\n",
    "    total = 0              \n",
    "    while total < 5:\n",
    "        t1 = time.time()\n",
    "        total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:03:46.368011Z",
     "start_time": "2018-01-17T02:03:46.348422Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_players = pd.DataFrame(dict_players)\n",
    "df_qb = df_players[df_players.pos == 'QB'].copy()\n",
    "df_qb.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:05:01.019779Z",
     "start_time": "2018-01-17T02:05:01.014664Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qb_url = df_qb.url.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T11:51:05.144954Z",
     "start_time": "2018-01-17T11:51:05.007662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_players.to_csv('nfl_players.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping NFL QB Stats\n",
    "* Using names from above scraping\n",
    "* Get player NFL stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T14:41:22.680652Z",
     "start_time": "2018-01-28T14:41:22.574612Z"
    }
   },
   "outputs": [],
   "source": [
    "df_qb = pd.read_csv('../nfl_players.csv')\n",
    "df_qb = df_qb[df_qb.pos == 'QB'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-01-28T14:41:23.829Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: make these nested if statements better...\n",
    "\n",
    "dict_player_year = {}\n",
    "dict_player_year = defaultdict(list)\n",
    "\n",
    "for i in range(len(df_qb)):\n",
    "    t0 = time.time()\n",
    "    url_nfl = 'https://www.pro-football-reference.com' + df_qb.url[i]\n",
    "\n",
    "    response_nfl=requests.get(url_nfl)\n",
    "    page_nfl=response_nfl.text\n",
    "\n",
    "    soup_nfl=BeautifulSoup(page_nfl,\"lxml\")\n",
    "\n",
    "    for row in soup_nfl.find_all(class_='full_table'):\n",
    "        if row.find('th',{'data-stat':'year_id'}).find('a') is not None:\n",
    "            if row.find('td',{'data-stat':'gs'}) is not None:\n",
    "                if row.find('td',{'data-stat':'pass_cmp'}) is not None:\n",
    "                    if row.find('td',{'data-stat':'pass_att'}) is not None:\n",
    "                        if row.find('td',{'data-stat':'pass_yds'}) is not None:\n",
    "                            if row.find('td',{'data-stat':'pass_td'}) is not None:\n",
    "                                if row.find('td',{'data-stat':'pass_int'}) is not None:\n",
    "                                    if row.find('td',{'data-stat':'team'}) is not None:\n",
    "                                        if row.find('td',{'data-stat':'g'}) is not None:\n",
    "                                            player = df_qb.player[i]\n",
    "                                            dict_player_year['player'].append(player)\n",
    "                                            dict_player_year['year'].append(row.find('th',{'data-stat':'year_id'}).find('a').text)\n",
    "                                            dict_player_year['games'].append(row.find('td',{'data-stat':'gs'}).text)\n",
    "                                            dict_player_year['pass_cmp'].append(row.find('td',{'data-stat':'pass_cmp'}).text)\n",
    "                                            dict_player_year['pass_att'].append(row.find('td',{'data-stat':'pass_att'}).text)\n",
    "                                            dict_player_year['pass_yds'].append(row.find('td',{'data-stat':'pass_yds'}).text)\n",
    "                                            dict_player_year['pass_td'].append(row.find('td',{'data-stat':'pass_td'}).text)\n",
    "                                            dict_player_year['team'].append(row.find('td',{'data-stat':'team'}).text)\n",
    "                                            dict_player_year['games_played'].append(row.find('td',{'data-stat':'g'}).text)\n",
    "\n",
    "    total = 0  \n",
    "    while total < 0:\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T21:43:48.723854Z",
     "start_time": "2018-01-19T21:43:48.672178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfl_stats.to_csv('nfl_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding CFB quarterbacks\n",
    "* Getting all CFB-Reference player names & URLs\n",
    "* Going to cross-reference this with NFL QB's to get final list\n",
    "* Little tricky to iterate through player pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_players_cfb = {}\n",
    "dict_players_cfb['player'] = []\n",
    "dict_players_cfb['url'] = []\n",
    "\n",
    "url_ext = ''\n",
    "i = 0\n",
    "letter = '`'\n",
    "\n",
    "while letter != {:\n",
    "    t0 = time.time()\n",
    "    url_ext = url_ext[:7] + '-' + str(i+1) + url_ext[-5:]\n",
    "    url_p = 'https://www.sports-reference.com/cfb/players/' + url_ext\n",
    "    response_p=requests.get(url_p)\n",
    "\n",
    "    if response_p.status_code != 200:\n",
    "        i = 0\n",
    "        letter = chr(ord(letter) + 1)\n",
    "        url_ext = letter + '-index.html'\n",
    "        url_p = 'https://www.sports-reference.com/cfb/players/' + url_ext\n",
    "        response_p=requests.get(url_p)\n",
    "    print(url_ext)\n",
    "    page_p=response_p.text\n",
    "    soup_p=BeautifulSoup(page_p,\"lxml\")\n",
    "\n",
    "    for row in soup_p.find_all('p'):\n",
    "        if row.find('a') is not None:\n",
    "            if row.find('a').get_attribute_list('href')[0][:13] == '/cfb/players/':\n",
    "                dict_players_cfb['player'].append(row.find('a').text)\n",
    "                dict_players_cfb['url'].append(row.find('a').get_attribute_list('href')[0])\n",
    "    i = i + 1\n",
    "    total = 0  \n",
    "    while total < 5:\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:44:23.708346Z",
     "start_time": "2018-01-17T13:44:23.702071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_players_cfb.to_csv('college_players.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare CFB & NFL Player lists\n",
    "* Only keep players from both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:48:41.941541Z",
     "start_time": "2018-01-17T21:48:41.364815Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_players_cfb = pd.read_csv('college_players.csv')\n",
    "df_players_nfl = pd.read_csv('nfl_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T21:48:42.868847Z",
     "start_time": "2018-01-17T21:48:42.730572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nfl_qb = df_players_nfl[df_players_nfl.pos == 'QB'].copy()\n",
    "df_both_qb = df_nfl_qb.merge(df_players_cfb, on='player').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting CFB Player Stats\n",
    "* Using combined list of QB names\n",
    "* Getting passing stats using BeautifulSoup\n",
    "* Getting rushing and receiving stats using Selenium w/ BeautifulSoup\n",
    "    * For some reason these tables are created separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T22:31:08.943454Z",
     "start_time": "2018-01-17T22:31:08.852196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nfl_qb['full_url'] = df_nfl_qb.apply(lambda x: 'https://www.pro-football-reference.com' + x['url'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T14:28:50.905130Z",
     "start_time": "2018-01-28T14:28:50.883850Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url_cfb_head = 'https://www.sports-reference.com' \n",
    "dict_cfb_career = defaultdict(list)\n",
    "    \n",
    "for index in df_both_qb.index:\n",
    "    t0 = time.time()\n",
    "    url_cfb = url_cfb_head + df_both_qb.url_y[index]\n",
    "    response_cfb=requests.get(url_cfb)\n",
    "    page_cfb=response_cfb.text\n",
    "\n",
    "    soup_cfb=BeautifulSoup(page_cfb,\"lxml\")\n",
    "\n",
    "    player = df_both_qb.player[index]\n",
    "\n",
    "    if soup_cfb.find('tbody') is not None:\n",
    "        if soup_cfb.find('table').get_attribute_list('id')[0] == 'passing':\n",
    "            tbl = soup_cfb.find('tbody')\n",
    "\n",
    "            for row in tbl.find_all('th'):\n",
    "                dict_cfb_career['player'].append(player)\n",
    "                dict_cfb_career[row['data-stat']].append(row.text)\n",
    "\n",
    "            for row in tbl.find_all('td'):\n",
    "                dict_cfb_career[row['data-stat']].append(row.text)\n",
    "\n",
    "    total = 0  \n",
    "    while total < 5:\n",
    "        t1 = time.time()\n",
    "        total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T11:51:59.046216Z",
     "start_time": "2018-01-18T11:51:59.006621Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_qb_passing = pd.DataFrame(dict_cfb_career)\n",
    "df_qb_passing.to_csv('cfb_qb_passing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T01:59:54.910412Z",
     "start_time": "2018-01-18T01:59:54.892154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import os\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T14:29:15.926965Z",
     "start_time": "2018-01-28T14:29:15.919071Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_cfb_head = 'https://www.sports-reference.com' \n",
    "dict_cfb_career_rush = defaultdict(list)\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "for index in df_both_qb.index:\n",
    "    t0 = time.time()\n",
    "    url_cfb = url_cfb_head + df_both_qb.url_y[index]\n",
    "\n",
    "    driver.get(url_cfb)\n",
    "\n",
    "    soup_cfb2 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    player = df_both_qb.player[index]\n",
    "    \n",
    "    if len(soup_cfb2.find_all('tbody')) > 2:\n",
    "        if soup_cfb2.find_all('table')[2].get_attribute_list('id')[0] == 'rushing':\n",
    "            tbl = soup_cfb2.find_all('tbody')[2]\n",
    "            \n",
    "            for row in tbl.find_all('th'):\n",
    "                dict_cfb_career_rush['player'].append(player)\n",
    "                dict_cfb_career_rush[row['data-stat']].append(row.text)\n",
    "\n",
    "            for row in tbl.find_all('td'):\n",
    "                dict_cfb_career_rush[row['data-stat']].append(row.text)\n",
    "    total = 0  \n",
    "    while total < 5:\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-18T11:50:59.374248Z",
     "start_time": "2018-01-18T11:50:59.192456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cfb_rush = pd.DataFrame(dict_cfb_career_rush)\n",
    "df_cfb_rush.to_csv('cfb_qb_rushing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Bio Scraping\n",
    "* Used Scrapy for learning purposes - BeautifulSoup may be easier\n",
    "* See NFL_Spider.py for scrapy script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
